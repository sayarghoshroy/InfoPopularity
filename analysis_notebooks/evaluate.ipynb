{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/userName/Salience-Prediction/blob/main/evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9uhvXuVarfmI"
   },
   "outputs": [],
   "source": [
    "# Various functions to evaluate methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ySK0UAiQrlUa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "import copy\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "42jXJLVYrrQ4"
   },
   "outputs": [],
   "source": [
    "dataset_select = 0\n",
    "# 0 for 'InfoPop, 1 for 'CNN_DailyMail'\n",
    "\n",
    "version = 1\n",
    "# 1 for ROUGE_1, 2 for ROUGE_2, 3 for ROUGE_L\n",
    "\n",
    "if dataset_select == 0:\n",
    "  dataset = 'InfoPop'\n",
    "else:\n",
    "  dataset = 'CNN_DailyMail'\n",
    "\n",
    "data_dir = 'drive/My Drive/store/'\n",
    "data_path = data_dir + dataset + '/' + 'test.json'\n",
    "\n",
    "with open(data_path, 'r+') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "use_LexRank = 0\n",
    "# Set 'use_LexRank' to 1\n",
    "# computed scores will be standardized to the 0 to 1 range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kK-VXllIA5Mx",
    "outputId": "91eed777-f718-46f4-81f4-02d693f36d45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of of test documents = 5177\n",
      "Number of test documents with number of sentences <= 5 = 44\n"
     ]
    }
   ],
   "source": [
    "# Find number of test documents with number of sentences <= threshold\n",
    "threshold = 5\n",
    "count = 0\n",
    "\n",
    "for doc in data:\n",
    "  if len(doc['sent_labels']) <= threshold:\n",
    "    count += 1\n",
    "\n",
    "print('Number of of test documents = ' + str(len(data)))\n",
    "print('Number of test documents with number of sentences <= ' + str(threshold) + ' = ' + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TMoyx6KUsLKR"
   },
   "outputs": [],
   "source": [
    "def position_baseline(sentence_set):\n",
    "  # Considers an ordered list of sentences\n",
    "  # Generates importance scores based on position\n",
    "  count = len(sentence_set)\n",
    "  normalizer = int((count * (count + 1)) / 2)\n",
    "  attach_scores = copy.deepcopy(sentence_set)\n",
    "\n",
    "  for index in range(count):\n",
    "    score = (count - index) / normalizer\n",
    "    attach_scores[index].append(score)\n",
    "    # for InfoPop unit[2] stores the predicted score\n",
    "    # 'unit' being a sentential unit containing the sentence string, importance score\n",
    "\n",
    "  return(attach_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KzIaTqXrtv47",
    "outputId": "2790bded-7ddf-43db-ebcf-500582ac2769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference outcome for the first sentence:\n",
      "['For years, hospitals and healthcare providers have blamed the countryâ€™s abysmal statistics on mothers for being too old, too fat or too unhealthy to have safe deliveries.', 0.03978408982232242, 0.022311022311022312]\n"
     ]
    }
   ],
   "source": [
    "# To test out implementation of position baseline\n",
    "test_position = 1\n",
    "if test_position == 1:\n",
    "  position_predicted = position_baseline(data[0]['sent_labels'])\n",
    "  print('Inference outcome for the first sentence:')\n",
    "  print(position_predicted[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XPRohUjPt8h7"
   },
   "outputs": [],
   "source": [
    "def get_splits(document_predicted):\n",
    "  global dataset_select, version, use_LexRank\n",
    "  actual_index = 1\n",
    "  predicted_index = 2\n",
    "\n",
    "  if dataset_select == 1:\n",
    "    actual_index = version\n",
    "    predicted_index = 4\n",
    "\n",
    "  actuals = []\n",
    "  predicted = []\n",
    "  for item in document_predicted:\n",
    "    actuals.append(item[actual_index])\n",
    "    predicted.append(item[predicted_index])\n",
    "  \n",
    "  actuals = np.asarray(actuals)\n",
    "  predicted = np.asarray(predicted)\n",
    "\n",
    "  if use_LexRank == 1:\n",
    "    total_computed = np.sum(predicted)\n",
    "    predicted = predicted / total_computed\n",
    "  \n",
    "  return actuals, predicted\n",
    "\n",
    "def MSE_score(document_predicted):\n",
    "  # Metric to calculate the MSE error for a single test document\n",
    "  actuals, predicted = get_splits(document_predicted)\n",
    "  MSE = sklearn.metrics.mean_squared_error(actuals, predicted)\n",
    "\n",
    "  return MSE\n",
    "\n",
    "def MAE_score(document_predicted):\n",
    "  # Metric to calculate the MAE error for a single test document\n",
    "  actuals, predicted = get_splits(document_predicted)\n",
    "  MAE = sklearn.metrics.mean_absolute_error(actuals, predicted)\n",
    "\n",
    "  return MAE\n",
    "\n",
    "def check_top(document_predicted):\n",
    "  # Metric to check if the sentence with the highest predicted score\n",
    "  # is indeed the highest scored sentence\n",
    "  actuals, predicted = get_splits(document_predicted)\n",
    "  actual_first = np.argmax(actuals)\n",
    "  predicted_first = np.argmax(predicted)\n",
    "\n",
    "  return int(actual_first == predicted_first)\n",
    "\n",
    "def create_check_overlap(k = 3):\n",
    "  def check_overlap(document_predicted):\n",
    "    # computes overlap between top k actual most important sentences\n",
    "    # and top k predicted most important sentences\n",
    "    actuals, predicted = get_splits(document_predicted)\n",
    "    if k > len(actuals):\n",
    "     return 1\n",
    "    \n",
    "    actuals_consider = np.argsort(actuals * (-1))[0: k]\n",
    "    predicted_consider = np.argsort(predicted * (-1))[0: k]\n",
    "\n",
    "    intersection = np.intersect1d(actuals_consider, predicted_consider)\n",
    "    score = len(intersection) / k\n",
    "    return score\n",
    "  \n",
    "  return check_overlap\n",
    "\n",
    "def kendall_tau(document_predicted):\n",
    "  # computes Kendall's tau's\n",
    "\n",
    "  actuals, predicted = get_splits(document_predicted)\n",
    "  actuals_rank = np.argsort(actuals * -1)\n",
    "  predicted_rank = np.argsort(predicted * -1)\n",
    "\n",
    "  score, _ = scipy.stats.kendalltau(actuals_rank, predicted_rank)\n",
    "  return score\n",
    "\n",
    "def spearman_rank(document_predicted):\n",
    "  # computes Spearman rank order correlation coefficient\n",
    "\n",
    "  actuals, predicted = get_splits(document_predicted)\n",
    "  actuals_rank = np.argsort(actuals * -1)\n",
    "  predicted_rank = np.argsort(predicted * -1)\n",
    "\n",
    "  score, _ = scipy.stats.spearmanr(actuals_rank, predicted_rank)\n",
    "  return score\n",
    "\n",
    "def get_true_relevance(score_list):\n",
    "  # relevance mapping for Normalized DCG computation\n",
    "  relevances = []\n",
    "\n",
    "  for score in score_list:\n",
    "    relevance = None\n",
    "    if score > 0.5:\n",
    "      relevance = 100\n",
    "    elif score > 0.1:\n",
    "      relevance = 50\n",
    "    elif score > 0.05:\n",
    "      relevance = 10\n",
    "    elif score > 0.01:\n",
    "      relevance = 1\n",
    "    else:\n",
    "      relevance = 0\n",
    "    relevances.append(relevance)\n",
    "  \n",
    "  return relevances\n",
    "    \n",
    "def ndcg_true_scores(document_predicted):\n",
    "  # computes Normalized DCG score\n",
    "  # based on true labels\n",
    "\n",
    "  actuals, predicted = get_splits(document_predicted)\n",
    "  \n",
    "  actuals_unit = [actuals]\n",
    "  predictions_unit = [predicted]\n",
    "\n",
    "  score = sklearn.metrics.ndcg_score(y_true = actuals_unit, y_score = predictions_unit)\n",
    "  return score\n",
    "\n",
    "def ndcg_relevance_scores(document_predicted):\n",
    "  # computes Normalized DCG score\n",
    "  # based on a defined scheme of relevance labelling\n",
    "\n",
    "  actuals, predicted = get_splits(document_predicted)\n",
    "  predictions_unit = [predicted]\n",
    "  \n",
    "  actual_scores = np.ndarray.tolist(actuals)\n",
    "  relevances = np.asarray([get_true_relevance(actual_scores)])\n",
    "\n",
    "  score = sklearn.metrics.ndcg_score(y_true = relevances, y_score = predictions_unit)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJj7EEUCw5Dj",
    "outputId": "06b519de-2ec1-4d2f-ccc1-27eef30494ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests on a single test document using the position baseline:\n",
      "- Outcome of running check_top(): 0\n",
      "- Outcome of running check_overlap(): 0.0\n",
      "- Outcome of running MSE_score(): 0.0011858518668143318\n",
      "- Outcome of running MAE_score(): 0.018615874990825915\n",
      "- Outcome of running kendall_tau(): 0.05673274094326725\n",
      "- Outcome of running spearman_rank(): 0.05762658394237341\n",
      "- Outcome of running ndcg_true_scores(): 0.4017899614712867\n",
      "- Outcome of running ndcg_relevance_scores(): 0.29373191277903143\n"
     ]
    }
   ],
   "source": [
    "# Test for implementation\n",
    "if test_position == 1:\n",
    "  print('Running tests on a single test document using the position baseline:')\n",
    "  print('- Outcome of running check_top(): ' + str(check_top(position_predicted)))\n",
    "  function = create_check_overlap(7)\n",
    "  print('- Outcome of running check_overlap(): ' + str(function(position_predicted)))\n",
    "  print('- Outcome of running MSE_score(): ' + str(MSE_score(position_predicted)))\n",
    "  print('- Outcome of running MAE_score(): ' + str(MAE_score(position_predicted)))\n",
    "  print('- Outcome of running kendall_tau(): ' + str(kendall_tau(position_predicted)))\n",
    "  print('- Outcome of running spearman_rank(): ' + str(spearman_rank(position_predicted)))\n",
    "  print('- Outcome of running ndcg_true_scores(): ' + str(ndcg_true_scores(position_predicted)))\n",
    "  print('- Outcome of running ndcg_relevance_scores(): ' + str(ndcg_relevance_scores(position_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yAbXRtiGw8bi"
   },
   "outputs": [],
   "source": [
    "def run_position_baseline(test_set):\n",
    "  # computes scores using the positon baseline\n",
    "  # on all test set documents\n",
    "  global dataset_select\n",
    "\n",
    "  computed = []\n",
    "  for unit in test_set:\n",
    "    item = {}\n",
    "    item['id'] = unit['id']\n",
    "    if dataset_select == 0:\n",
    "      item['url'] = unit['url']\n",
    "    item['sent_labels'] = position_baseline(unit['sent_labels'])\n",
    "    computed.append(item)\n",
    "  return computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AsJsuS31yiEE"
   },
   "outputs": [],
   "source": [
    "# Run position baseline on all samples\n",
    "run_all_position = 1\n",
    "if run_all_position == 1:\n",
    "  position_scores = run_position_baseline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sl6C93kAyuQP"
   },
   "outputs": [],
   "source": [
    "def get_total_score(predicted, scoring_function = check_top):\n",
    "  scores = []\n",
    "  \n",
    "  for unit in predicted:\n",
    "    score = scoring_function(unit['sent_labels'])\n",
    "    scores.append(score)\n",
    "  scores = np.asarray(scores)\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kHEzc5vz59Z",
    "outputId": "d8b1a0ad-e97f-464d-dc8e-18dd9dbd768a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the full test set using the position baseline\n",
      "- Average highest importance prediction: 6.9152018543558045 %\n",
      "Top k scores for k = 1(1)20:\n",
      "[6.9152018543558045, 10.80741742321808, 16.45740776511493, 22.21363724164574, 26.94610778443114, 31.366299658747028, 35.434200722978005, 38.866621595518644, 41.707981885690984, 44.44465906895885, 46.98579380827786, 49.43178159809414, 51.67976701683482, 53.93774662656254, 56.11100379885391, 58.29631060459726, 60.47336067902147, 62.455733693902516, 64.48969632890416]\n",
      "- Average MSE: 0.007858691173335416\n",
      "- Average MAE: 0.053011265403347406\n",
      "- Average Kendall's Tau: 0.03339373658300352\n",
      "- Average Spearman's Rank Correlation Coefficient: 0.0424231412389899\n",
      "- Average nDCG based on true labels: 0.5803590013865478\n",
      "- Average nDCG based on synthetic relevance labels: 0.5550813643685166\n"
     ]
    }
   ],
   "source": [
    "if run_all_position == 1 and use_LexRank == 0:\n",
    "  print('Results on the full test set using the position baseline')\n",
    "\n",
    "  print('- Average highest importance prediction: ', end = '')\n",
    "  print(get_total_score(position_scores) * 100, end = '')\n",
    "  print(' %')\n",
    "\n",
    "  top_K = []\n",
    "\n",
    "  for k in range(1, 20):\n",
    "    function = create_check_overlap(k)\n",
    "    top_K.append(get_total_score(position_scores, scoring_function = function) * 100)\n",
    "\n",
    "  print('Top k scores for k = 1(1)20:')\n",
    "  print(top_K)\n",
    "\n",
    "  print('- Average MSE: ', end = '')\n",
    "  print(get_total_score(position_scores, scoring_function = MSE_score))\n",
    "  \n",
    "  print('- Average MAE: ', end = '')\n",
    "  print(get_total_score(position_scores, scoring_function = MAE_score))\n",
    "\n",
    "  print('- Average Kendall\\'s Tau: ', end = '')\n",
    "  print(get_total_score(position_scores, scoring_function = kendall_tau))\n",
    "\n",
    "  print('- Average Spearman\\'s Rank Correlation Coefficient: ', end = '')\n",
    "  print(get_total_score(position_scores, scoring_function = spearman_rank))\n",
    "\n",
    "  print('- Average nDCG based on true labels: ', end = '')\n",
    "  print(get_total_score(position_scores, scoring_function = ndcg_true_scores))\n",
    "\n",
    "  print('- Average nDCG based on synthetic relevance labels: ', end = '')\n",
    "  print(get_total_score(position_scores, scoring_function = ndcg_relevance_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ldGMHaOLTj76"
   },
   "outputs": [],
   "source": [
    "def get_metrics(name, dataset):\n",
    "  results = [name]\n",
    "  results.append(get_total_score(dataset) * 100)\n",
    "\n",
    "  top_K = []\n",
    "\n",
    "  for k in range(1, 20):\n",
    "    function = create_check_overlap(k)\n",
    "    top_K.append(get_total_score(dataset, scoring_function = function) * 100)\n",
    "\n",
    "  results.append(top_K)\n",
    "  results.append(get_total_score(dataset, scoring_function = MSE_score))\n",
    "  results.append(get_total_score(dataset, scoring_function = MAE_score))\n",
    "  results.append(get_total_score(dataset, scoring_function = kendall_tau))\n",
    "  results.append(get_total_score(dataset, scoring_function = spearman_rank))\n",
    "  results.append(get_total_score(dataset, scoring_function = ndcg_true_scores))\n",
    "  results.append(get_total_score(dataset, scoring_function = ndcg_relevance_scores))\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZfiKYGhSBUW",
    "outputId": "8ed7a975-a93e-4c17-b5f4-4f56edf7b24a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:51<00:00, 17.19s/it]\n"
     ]
    }
   ],
   "source": [
    "if use_LexRank == 0:\n",
    "  model_name = 'SummaRunner-Reg'\n",
    "  location = data_dir + model_name + '/' + dataset + '/'\n",
    "  results_dump = []\n",
    "\n",
    "  for unit in tqdm(os.listdir(location)):\n",
    "    if os.path.isdir(location + unit) == False:\n",
    "      continue\n",
    "\n",
    "    with open(location + unit + '/' + 'batchpred.json', 'r+') as f:\n",
    "      data = json.load(f)\n",
    "      results = get_metrics(unit, data)\n",
    "      results_dump.append(results)\n",
    "      \n",
    "  name = model_name + '_' + dataset\n",
    "  if dataset_select == 1:\n",
    "    name += ('_oracle_' + str(version))\n",
    "\n",
    "  with open(name + '.json', 'w+') as w:\n",
    "    json.dump(results_dump, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mzH88yzZtxp3"
   },
   "outputs": [],
   "source": [
    "if use_LexRank == 1:\n",
    "  model_name = 'LexRank'\n",
    "  location = data_dir + model_name\n",
    "\n",
    "  dump_name = location + '/' + dataset\n",
    "\n",
    "  with open(dump_name + '.json', 'r+') as f:\n",
    "    data = json.load(f)\n",
    "    results = get_metrics(model_name, data)\n",
    "\n",
    "  name = model_name + '_' + dataset\n",
    "  if dataset_select == 1:\n",
    "    name += ('_oracle_' + str(version))\n",
    "\n",
    "  with open(name + '.json', 'w+') as w:\n",
    "    json.dump(results, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eVsiWrEB8mjj"
   },
   "outputs": [],
   "source": [
    "if use_LexRank == 0:\n",
    "  model_name = 'position'\n",
    "  results = get_metrics(model_name, position_scores)\n",
    "\n",
    "  name = model_name + '_' + dataset\n",
    "  if dataset_select == 1:\n",
    "    name += ('_oracle_' + str(version))\n",
    "\n",
    "  with open(name + '.json', 'w+') as w:\n",
    "    json.dump(results, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qLHzzy450AWu"
   },
   "outputs": [],
   "source": [
    "# ^_^ Thank You"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO6zDG5BLr4o0uHbQI3r8GL",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1Pt5nYJivWoBup7fLlIDbOTpkTJNEHIoQ",
   "name": "evaluate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
