{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/userName/Salience-Prediction/blob/main/InfoPop_data_build.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MJOXv27ZkSMP"
   },
   "outputs": [],
   "source": [
    "# Builds and saves the InfoPop dataset with Train, Validation and Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ug3kcLgNkeI0"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import nltk\n",
    "import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YogBKtckklmf",
    "outputId": "b5b72f2e-7f2b-4d49-a2d4-b4d8d20105fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Resources\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YxnAKvTlksFt"
   },
   "outputs": [],
   "source": [
    "file_name = 'drive/My Drive/summworth/cleaned_document.json'\n",
    "\n",
    "# Loading Created Pre-processed data\n",
    "with open(file_name, 'r+') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IK3NNUC9lr-x",
    "outputId": "7cf98d07-70ec-4715-9b7f-3e8e7fa33a24",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the final data with explicit limits\n",
    "final_set = []\n",
    "sentence_token_count_limit = 40\n",
    "total_sentence_count_upper = 100\n",
    "total_sentence_count_lower = 2\n",
    "\n",
    "# Stores the number of sentences with importance as 0 for each document\n",
    "# Format: (Total Number of Sentences, Number of Sentences with Score 0)\n",
    "scores_stats = []\n",
    "\n",
    "too_small = 0\n",
    "too_large = 0\n",
    "not_a_number_count = 0\n",
    "not_important = 0\n",
    "\n",
    "def count_tokens(sentence):\n",
    "  tokens = nltk.word_tokenize(sentence)\n",
    "  return len(tokens)\n",
    "\n",
    "def get_score_stats(document):\n",
    "  total = len(document['sent_labels'])\n",
    "  count = 0\n",
    "  for sentence in document['sent_labels']:\n",
    "    if float(sentence[1]) == 0.0:\n",
    "      continue\n",
    "    count += 1\n",
    "  return [total, count]\n",
    "\n",
    "def normalize_scores(document):\n",
    "  sum_of_scores = 0\n",
    "  for sentence in document['sent_labels']:\n",
    "    sum_of_scores += float(sentence[1])\n",
    "  \n",
    "  normalized_labels = []\n",
    "  for sentence in document['sent_labels']:\n",
    "    new_score = float(sentence[1]) / sum_of_scores\n",
    "    normalized_labels.append([sentence[0], new_score])\n",
    "  \n",
    "  normalized_doc = document\n",
    "  normalized_doc['sent_labels'] = normalized_labels\n",
    "  return normalized_doc\n",
    "\n",
    "value_threshold = 5\n",
    "too_large_sentences = 0\n",
    "too_large_cases = []\n",
    "\n",
    "for key, value in tqdm.tqdm(data.items()):\n",
    "  not_a_number_flag = 0\n",
    "  too_large_value_flag = 0\n",
    "\n",
    "  units = data[key]['sent_labels']\n",
    "  processed_units = data[key]\n",
    "  processed_units['sent_labels'] = []\n",
    "  count = 0\n",
    "  for unit in units:\n",
    "    if len(unit) != 3 or len(unit[0]) < 2 or unit[2] != 1 or count_tokens(unit[0]) > sentence_token_count_limit:\n",
    "      continue\n",
    "\n",
    "    # Verifying number of sentences with too large scores\n",
    "    if float(unit[1]) > value_threshold:\n",
    "      too_large_value_flag = 1\n",
    "      too_large_sentences += 1\n",
    "      too_large_cases.append([data[key]['id'], unit[0], float(unit[1])])\n",
    "\n",
    "    if math.isnan(float(unit[1])):\n",
    "      not_a_number_flag = 1\n",
    "      break\n",
    "    count += 1\n",
    "    processed_units['sent_labels'].append(unit)\n",
    "  \n",
    "  if not_a_number_flag == 1:\n",
    "    not_a_number_count += 1\n",
    "  if count > total_sentence_count_upper:\n",
    "    too_large += 1\n",
    "    continue\n",
    "  if count < total_sentence_count_lower:\n",
    "    too_small += 1\n",
    "    continue\n",
    "  \n",
    "  if not_a_number_flag == 0:\n",
    "    importance_score_stats = get_score_stats(processed_units)\n",
    "    if importance_score_stats[1] > 2: \n",
    "      final_set.append(normalize_scores(processed_units))\n",
    "      scores_stats.append(importance_score_stats + [{'id': processed_units['id'], 'url': processed_units['url']}])\n",
    "    else:\n",
    "      not_important += 1\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('• Documents which were:')\n",
    "print('  - Too Small: ' + str(too_small))\n",
    "print('  - Too Large: ' + str(too_large))\n",
    "print('• Documents containing NaN as an importance labels: ' + str(not_a_number_count))\n",
    "print()\n",
    "print('After pruning the above cases:')\n",
    "print('• Documents containing less than three popular sentences: ' + str(not_important))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEJeJdWtWWrh",
    "outputId": "a013d4e9-8183-48c3-eb39-b89900a4e58a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing the Scores Statistics List:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[72,\n",
       "  12,\n",
       "  {'id': 2, 'url': 'http://cnn.com/2009/opinion/10/26/opinion.jonathan.foer'}],\n",
       " [95,\n",
       "  4,\n",
       "  {'id': 3,\n",
       "   'url': 'http://cnn.com/2013/02/19/world/africa/south-africa-pistorius-case/index.html'}],\n",
       " [10,\n",
       "  3,\n",
       "  {'id': 5,\n",
       "   'url': 'http://cs.thomsonreuters.com/ua/acct_pr/acs/cs_us_en/common/com_proc/workflows-overview.htm'}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the Scores Statistics List\n",
    "print('Viewing the Scores Statistics List:')\n",
    "scores_stats[0: 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CS4sNQEipupE",
    "outputId": "4528c021-1eb3-4d16-9708-9964c314663e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Datapoints: 51770\n",
      "Train Size: 41416\n",
      "Val Size: 5177\n",
      "Test Size: 5177\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into Train, Validation and Test Splits\n",
    "\n",
    "indices = [id for id in range(len(final_set))]\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_size = int((8 / 10) * len(final_set))\n",
    "val_size = int((1 / 10) * len(final_set))\n",
    "test_size = len(final_set) - (train_size + val_size)\n",
    "\n",
    "print('Total Number of Datapoints: ' + str(len(final_set)))\n",
    "print('Train Size: ' + str(train_size))\n",
    "print('Val Size: ' + str(val_size))\n",
    "print('Test Size: ' + str(test_size))\n",
    "\n",
    "train_indices = set(indices[0: train_size])\n",
    "val_indices = set(indices[train_size: train_size + val_size])\n",
    "test_indices = set(indices[train_size + val_size: ])\n",
    "\n",
    "train, val, test = [], [], []\n",
    "\n",
    "for index in indices:\n",
    "  if index in train_indices:\n",
    "    train.append(final_set[index])\n",
    "  elif index in val_indices:\n",
    "    val.append(final_set[index])\n",
    "  else:\n",
    "    test.append(final_set[index])\n",
    "\n",
    "# Splits created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SCWgfU-wh7wL"
   },
   "outputs": [],
   "source": [
    "# Uncomment to View Examples\n",
    "# train[1]['sent_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1fifKXzL6joT"
   },
   "outputs": [],
   "source": [
    "# Removing redundant labels for Grammaticality\n",
    "\n",
    "def drop_label(dump):\n",
    "  new_dump = []\n",
    "\n",
    "  for document in dump:\n",
    "    new_unit = {}\n",
    "    new_unit['id'] = document['id']\n",
    "    new_unit['url'] = document['url']\n",
    "    sentence_labels = []\n",
    "    old_labels = document['sent_labels']\n",
    "\n",
    "    for sentence in old_labels:\n",
    "      sentence_labels.append(sentence[0: 2])\n",
    "    new_unit['sent_labels'] = sentence_labels\n",
    "\n",
    "    new_dump.append(new_unit)\n",
    "\n",
    "  return new_dump\n",
    "\n",
    "train = drop_label(train)\n",
    "val = drop_label(val)\n",
    "test = drop_label(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gr1xZtoIrcs5"
   },
   "outputs": [],
   "source": [
    "# Saving Processed Files\n",
    "location = 'drive/My Drive/store/InfoPop/'\n",
    "\n",
    "with open(location + 'stats.json', 'w+') as f:\n",
    "  json.dump(scores_stats, f)\n",
    "\n",
    "with open(location + 'train.json', 'w+') as f:\n",
    "  json.dump(train, f)\n",
    "\n",
    "with open(location + 'val.json', 'w+') as f:\n",
    "  json.dump(val, f)\n",
    "\n",
    "with open(location + 'test.json', 'w+') as f:\n",
    "  json.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hWaT1O26tdqG"
   },
   "outputs": [],
   "source": [
    "# ^_^ Thank You"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPzbbKYMxmftHkM8qe3uer1",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1yNvE4MyU_pq9eYfDgtLMPs9cjFZkx2Q1",
   "name": "InfoPop_data_build.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
