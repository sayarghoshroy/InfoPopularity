{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/userName/Salience-Prediction/blob/main/get_stats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "X8aHvLqbDOZx"
   },
   "outputs": [],
   "source": [
    "# Get various statistics on complete InfoPop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XDP2WG7GDnFv"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fetching resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "valid_POS = ['FW', 'JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS',\n",
    "             'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'UNK']\n",
    "be_verbs = ['be', 'is', 'was', 'were']\n",
    "\n",
    "def get_content_ratio(sentence):\n",
    "    tokenized_set = nltk.word_tokenize(sentence)\n",
    "    tagged_map = nltk.pos_tag(tokenized_set)\n",
    "\n",
    "    count_content = 0\n",
    "    count_total = len(tokenized_set)\n",
    "\n",
    "    for elem in tagged_map:\n",
    "      checker = elem[1]\n",
    "      if checker in valid_POS and checker != 'POS' and elem[0] not in be_verbs:\n",
    "          count_content += 1\n",
    "          \n",
    "    ratio = count_content / count_total\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DBzlDJ_EDpf7"
   },
   "outputs": [],
   "source": [
    "data_dir = 'drive/My Drive/store/InfoPop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "P53msfr0DxXY"
   },
   "outputs": [],
   "source": [
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "data = []\n",
    "\n",
    "for split in splits:\n",
    "  with open(data_dir + '/' + split + '.json', 'r+') as f:\n",
    "    temp_data = json.load(f)\n",
    "    data += temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hsJaNqcEMV1",
    "outputId": "060f1c00-da4f-49bd-b0c1-b3a2f1e3ea8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints: 51770\n"
     ]
    }
   ],
   "source": [
    "print('Number of datapoints: ' + str(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJnGaeiOEUce",
    "outputId": "52b80746-25d1-4ec2-a8c3-7a28c95a26ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class frequencies:\n",
      "[3103, 11822, 12110, 10041, 6267, 3675, 2096, 1272, 839, 545]\n",
      "\n",
      "Sum of class frequencies: 51770\n"
     ]
    }
   ],
   "source": [
    "sentence_counts = []\n",
    "for unit in data:\n",
    "  count = len(unit['sent_labels'])\n",
    "  sentence_counts.append(count)\n",
    "\n",
    "with open('sent_counts.json', 'w+') as f:\n",
    "  json.dump(sentence_counts, f)\n",
    "\n",
    "freqs = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# Class frequencies\n",
    "for item in sentence_counts:\n",
    "\tif item <= 10:\n",
    "\t\tfreqs[0] += 1\n",
    "\tif item > 10 and item <= 20:\n",
    "\t\tfreqs[1] += 1\n",
    "\tif item > 20 and item <= 30:\n",
    "\t\tfreqs[2] += 1\n",
    "\tif item > 30 and item <= 40:\n",
    "\t\tfreqs[3] += 1\n",
    "\tif item > 40 and item <= 50:\n",
    "\t\tfreqs[4] += 1\n",
    "\tif item > 50 and item <= 60:\n",
    "\t\tfreqs[5] += 1\n",
    "\tif item > 60 and item <= 70:\n",
    "\t\tfreqs[6] += 1\n",
    "\tif item > 70 and item <= 80:\n",
    "\t\tfreqs[7] += 1\n",
    "\tif item > 80 and item <= 90:\n",
    "\t\tfreqs[8] += 1\n",
    "\tif item > 90 and item <= 100:\n",
    "\t\tfreqs[9] += 1\n",
    "\n",
    "print('Class frequencies:')\n",
    "print(freqs)\n",
    "\n",
    "sentence_counts = np.asarray(sentence_counts)\n",
    "print()\n",
    "print('Sum of class frequencies: ' + str(sum(freqs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLKme9-KEv6s",
    "outputId": "00776023-b1a1-47a9-fb8d-851064824864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on number of sentences in each article:\n",
      "- Minimum: 3\n",
      "- Maxmimum: 100\n",
      "- Average: 33.06722039791385\n",
      "\n",
      "Total number of sentences within the dataset: 1711890\n"
     ]
    }
   ],
   "source": [
    "print('Statistics on number of sentences in each article:')\n",
    "print('- Minimum: ' + str(np.min(sentence_counts)))\n",
    "print('- Maxmimum: ' + str(np.max(sentence_counts)))\n",
    "print('- Average: ' + str(np.mean(sentence_counts)))\n",
    "print()\n",
    "print('Total number of sentences within the dataset: ' + str(np.sum(sentence_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlJeiV2uGjPo",
    "outputId": "61f09a2e-6fe2-4bae-f9d7-2974bdbba7a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51770/51770 [38:54<00:00, 22.18it/s]\n"
     ]
    }
   ],
   "source": [
    "sent_token_counts = []\n",
    "sent_scores_list = []\n",
    "sent_lexical_densities = []\n",
    "\n",
    "document_token_counts = []\n",
    "\n",
    "for unit in tqdm(data):\n",
    "  doc_count = 0\n",
    "  for sentence_unit in unit['sent_labels']:\n",
    "    sentence = sentence_unit[0]\n",
    "    sent_score = sentence_unit[1]\n",
    "    content_ratio = get_content_ratio(sentence)\n",
    "    \n",
    "    token_count = len(nltk.word_tokenize(sentence))\n",
    "    doc_count += token_count\n",
    "    \n",
    "    sent_token_counts.append(token_count)\n",
    "    sent_scores_list.append(sent_score)\n",
    "    sent_lexical_densities.append(content_ratio)\n",
    "    \n",
    "  document_token_counts.append(doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hFls4dpVxCSA"
   },
   "outputs": [],
   "source": [
    "# Saving datapoints\n",
    "\n",
    "with open('scores.json', 'w+') as f:\n",
    "  json.dump(sent_scores_list, f)\n",
    "\n",
    "with open('lengths.json', 'w+') as f:\n",
    "  json.dump(sent_token_counts, f)\n",
    "\n",
    "with open('lex_den.json', 'w+') as f:\n",
    "  json.dump(sent_lexical_densities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pf891DZx1xz",
    "outputId": "18dac8f4-9622-4d14-a9ba-860e3644d06f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient between: \n",
      "- Scores and sentence lengths: 0.1680641497724356\n",
      "- Normalized scores and sentence lengths: -0.04397969664402135\n",
      "- Scores and sentence lexical densities: 0.059994910660698175\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "\n",
    "def div(a, b):\n",
    "  div_list = []\n",
    "  for index, item in enumerate(a):\n",
    "    div_list.append(item / b[index])\n",
    "  \n",
    "  return div_list\n",
    "\n",
    "assert len(sent_scores_list) == len(sent_token_counts)\n",
    "assert len(sent_scores_list) == len(sent_lexical_densities)\n",
    "\n",
    "print('Correlation coefficient between: ')\n",
    "print('- Scores and sentence lengths: ' + str(np.corrcoef(sent_scores_list, sent_token_counts)[0, 1]))\n",
    "print('- Normalized scores and sentence lengths: ' + str(np.corrcoef( sent_token_counts, div(sent_scores_list, sent_token_counts))[0, 1]))\n",
    "print('- Scores and sentence lexical densities: ' + str(np.corrcoef(sent_scores_list, sent_lexical_densities)[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iktJ8vBGH-Hp",
    "outputId": "d558dc48-577a-4383-fb01-7c60ef123bd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on number of tokens in each sentence:\n",
      "- Minimum: 1\n",
      "- Maxmimum: 40\n",
      "- Average: 18.23310025761001\n",
      "\n",
      "Total number of tokens within the dataset: 31213062\n"
     ]
    }
   ],
   "source": [
    "# Stats on tokens within sentences\n",
    "print('Statistics on number of tokens in each sentence:')\n",
    "print('- Minimum: ' + str(np.min(sent_token_counts)))\n",
    "print('- Maxmimum: ' + str(np.max(sent_token_counts)))\n",
    "print('- Average: ' + str(np.mean(sent_token_counts)))\n",
    "print()\n",
    "print('Total number of tokens within the dataset: ' + str(np.sum(sent_token_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnximxJnIzfg",
    "outputId": "095b7ddd-1f2f-48e8-fada-c383abef8c5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on number of tokens in each document:\n",
      "- Minimum: 15\n",
      "- Maxmimum: 2516\n",
      "- Average: 602.91794475565\n",
      "\n",
      "Total number of tokens within the dataset: 31213062\n"
     ]
    }
   ],
   "source": [
    "# Stats on tokens within documents\n",
    "print('Statistics on number of tokens in each document:')\n",
    "print('- Minimum: ' + str(np.min(document_token_counts)))\n",
    "print('- Maxmimum: ' + str(np.max(document_token_counts)))\n",
    "print('- Average: ' + str(np.mean(document_token_counts)))\n",
    "print()\n",
    "print('Total number of tokens within the dataset: ' + str(np.sum(document_token_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z7dIghbQOIxE",
    "outputId": "4cb94924-ba9a-44e9-c394-804f37561e8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs having multiple matches: 319\n",
      "Number of URLs that do not fit it into any bucket: 0\n"
     ]
    }
   ],
   "source": [
    "# Getting stats on the various sourcing websites\n",
    "urls = []\n",
    "\n",
    "for unit in data:\n",
    "  urls.append(unit['url'])\n",
    "\n",
    "primary_sites = ['hindustantimes', 'timesnownews', 'theguardian', 'techcrunch', 'livemint', 'crictracker', 'phys',\n",
    "                 'inshorts', 'cnn', 'nytimes', 'huffingtonpost', 'foxnews', 'reuters', 'usatoday', 'npr', 'latimes',\n",
    "                 'nbcnews', 'cbsnews', 'nypost', 'nydailynews', 'abcnews.go', 'newsweek', 'denverpost',\n",
    "                 'washington.cbslocal', 'sanfrancisco.cbslocal', 'chicagotribune', 'cbslocal']\n",
    "\n",
    "primary_sites.sort(key = len, reverse = True)\n",
    "# To ensure that substrings of larger string units appear after the larger string\n",
    "\n",
    "freq_count = {}\n",
    "\n",
    "for site in primary_sites:\n",
    "  freq_count[site] = 0\n",
    "\n",
    "unfits = []\n",
    "multiple_matches = []\n",
    "\n",
    "for url in urls:\n",
    "  flag = 0\n",
    "  for site in primary_sites:\n",
    "    if site in url:\n",
    "      if flag == 1:\n",
    "        if len(multiple_matches) == 0:\n",
    "          multiple_matches.append(url)\n",
    "        if len(multiple_matches) > 0 and multiple_matches[-1] != url:\n",
    "          multiple_matches.append(url)\n",
    "      else:\n",
    "        flag = 1\n",
    "        freq_count[site] += 1\n",
    "  if flag == 0:\n",
    "    unfits.append(url)\n",
    "\n",
    "print('Number of URLs having multiple matches: ' + str(len(multiple_matches)))\n",
    "print('Number of URLs that do not fit it into any bucket: ' + str(len(unfits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6LPdjo0RcXw",
    "outputId": "31f97716-5622-4221-c570-b944d58d50c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles sourced from each website: \n",
      "- sanfrancisco.cbslocal: 146\n",
      "- washington.cbslocal: 40\n",
      "- hindustantimes: 371\n",
      "- huffingtonpost: 75\n",
      "- chicagotribune: 1038\n",
      "- timesnownews: 189\n",
      "- theguardian: 142\n",
      "- crictracker: 44\n",
      "- nydailynews: 1453\n",
      "- techcrunch: 1064\n",
      "- abcnews.go: 1490\n",
      "- denverpost: 897\n",
      "- livemint: 411\n",
      "- usatoday: 6570\n",
      "- newsweek: 1806\n",
      "- cbslocal: 2522\n",
      "- nytimes: 10677\n",
      "- foxnews: 2388\n",
      "- reuters: 2616\n",
      "- latimes: 1917\n",
      "- nbcnews: 2346\n",
      "- cbsnews: 4979\n",
      "- nypost: 2509\n",
      "- phys: 1190\n",
      "- cnn: 1638\n",
      "- npr: 3252\n",
      "\n",
      "Number of website(s) from which no URL was sourced: 1\n",
      "Website(s) from which no URL was sourced: ['inshorts']\n",
      "\n",
      "Number of websites from which URLs were sourced: 26\n",
      "Average number of URLs sourced from a single website: 1991.1538461538462\n",
      "Sanity check ~ Total number of sourced URLs: 51770\n"
     ]
    }
   ],
   "source": [
    "print('Number of articles sourced from each website: ')\n",
    "\n",
    "website_article_counts = []\n",
    "zero_maps = []\n",
    "\n",
    "for key in freq_count:\n",
    "  if freq_count[key] == 0:\n",
    "    zero_maps.append(key)\n",
    "    continue\n",
    "  print('- ' + key + ': ' + str(freq_count[key]))\n",
    "  website_article_counts.append(freq_count[key])\n",
    "\n",
    "print()\n",
    "print('Number of website(s) from which no URL was sourced: ' + str(len(zero_maps)))\n",
    "print('Website(s) from which no URL was sourced: ' + str(zero_maps))\n",
    "print()\n",
    "\n",
    "website_article_counts = np.asarray(website_article_counts)\n",
    "print('Number of websites from which URLs were sourced: ' + str(len(website_article_counts)))\n",
    "print('Average number of URLs sourced from a single website: ' + str(np.mean(website_article_counts)))\n",
    "print('Sanity check ~ Total number of sourced URLs: ' + str(np.sum(website_article_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Ag0Z0JTlFAV6"
   },
   "outputs": [],
   "source": [
    "# Thank You ^_^"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP5DxFMiFzGvnGbLksECTyh",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1XRhukkH65muycKD8dZYCtOb1S0CBzJXi",
   "name": "get_stats.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
